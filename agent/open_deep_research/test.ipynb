{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b0c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.open_deep_research.configuration import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAPI(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    OPENAI = \"openai\"\n",
    "    TAVILY = \"tavily\"\n",
    "\n",
    "    NONE= \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPConfig(BaseModel):\n",
    "    url: Optional[str] = Field(\n",
    "        default = None,\n",
    "        description=\"The URL of the MCP Server\"\n",
    "        )\n",
    "\n",
    "    tools: Optional[str] = Field(\n",
    "        default= False,\n",
    "        description=\"The tools to make available to the LLM\"\n",
    "    )\n",
    "\n",
    "    auth_required: Optional[bool] = Field(\n",
    "        default = False,\n",
    "        description = \"Whether the MCP server requires authentication\"\n",
    "    )\n",
    "\n",
    "\n",
    "class configuration(BaseModel):\n",
    "\n",
    "    max_structured_output_retries: int = Field(\n",
    "        default = 3,\n",
    "        metadata = {\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\" : 3,\n",
    "                \"min\": 1,\n",
    "                \"max\": 10,\n",
    "                \"description\" : \"Maximum number of retries for structured output calls from models\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    allow_clarification: bool = Field(\n",
    "        default = True, \n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"default\": True,\n",
    "                \"description\": \"Whether to allow the researcher to ask the user clarifying questions before starting research\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    max_concurrent_research_units: int = Field(\n",
    "        default = 5,\n",
    "        metadata = {\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"slider\",\n",
    "                \"default\": 5,\n",
    "                \"min\": 1,\n",
    "                \"max\": 20,\n",
    "                \"step\": 1,\n",
    "                \"description\": \"Maximum number of research units to run concurrently. This will allow the researcher to use multiple sub-agents to conduct research. Note: with more concurrency, you may run into rate limits\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Research Configuration\n",
    "    search_api: SearchAPI = Field(\n",
    "        default=SearchAPI.TAVILY,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"select\",\n",
    "                \"default\": \"tavily\",\n",
    "                \"description\": \"Search API to use for research. NOTE: Make sure your Researcher Model supports the selected search API.\",\n",
    "                \"options\": [\n",
    "                    {\"label\": \"Tavily\", \"value\": SearchAPI.TAVILY.value},\n",
    "                    {\"label\": \"OpenAI Native Web Search\", \"value\": SearchAPI.OPENAI.value},\n",
    "                    {\"label\": \"Anthropic Native Web Search\", \"value\": SearchAPI.ANTHROPIC.value},\n",
    "                    {\"label\": \"None\", \"value\": SearchAPI.NONE.value}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    max_react_tool_calls: int = Field(\n",
    "        default = 5,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"slider\",\n",
    "                \"default\": 5,\n",
    "                \"min\": 1,\n",
    "                \"max\": 30,\n",
    "                \"step\": 1,\n",
    "                \"description\": \"Maximum number of tool calling iterations to make in a single researcher step.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    summarization_model: str = Field(\n",
    "        default = \"openai:gpt-4.1-nano\",\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": \"openai:gpt-4.1-nano\",\n",
    "                \"description\": \"Model for summarizing research results from Tavily search results\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    summarization_model_max_tokens: int = Field(\n",
    "        default = 8192,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 8192,\n",
    "                \"description\": \"Maximum output tokens for summarization model\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    research_model: str = Field(\n",
    "        default=\"openai:gpt-4.1\",\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": \"openai:gpt-4.1\",\n",
    "                \"description\": \"Model for conducting research. NOTE: Make sure your Researcher Model supports the selected search API.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    reseach_model_max_tokens: int = Field(\n",
    "        default = 10000,\n",
    "                metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 10000,\n",
    "                \"description\": \"Maximum output tokens for research model\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    compression_model: str = Field(\n",
    "        default = \"openai:gpt-4.1-mini\",\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": \"openai:gpt-4.1-mini\",\n",
    "                \"description\": \"Model for compressing research findings from sub-agents. NOTE: Make sure your Compression Model supports the selected search API.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    compression_model_max_tokens: int = Field(\n",
    "        default = 8192,\n",
    "        metadata = {\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 8192,\n",
    "                \"description\": \"Maximum output tokens for compression model\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    final_report_model: str = Field(\n",
    "        default=\"openai:gpt-4.1\",\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": \"openai:gpt-4.1\",\n",
    "                \"description\": \"Model for writing the final report from all research findings\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    final_report_model_max_tokens: int = Field(\n",
    "        default=10000,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 10000,\n",
    "                \"description\": \"Maximum output tokens for final report model\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mcp_config: Optional[MCPConfig] = Field(\n",
    "        default = None,\n",
    "        optional=True,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"mcp\",\n",
    "                \"description\": \"MCP server configuration\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mcp_prompt: Optional[str] = Field(\n",
    "        default=None,\n",
    "        optional=True,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"description\": \"Any additional instructions to pass along to the Agent regarding the MCP tools that are available to it.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\": \n",
    "    \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"ArithmeticError\n",
    "        configurable = config.get(\"configurable\", {}) if config else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcee17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import MessageLikeRepresentation\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConductResearch(BaseModel):\n",
    "    \"\"\"Call this tool to conduct research on a specific topic.\"\"\"\n",
    "    research_topic: str = Field(\n",
    "        description = \"The topic to research. Should be a single topic, and should be described in high detail (at least a paragraph).\",\n",
    "    )\n",
    "\n",
    "class ResearchComplete(BaseModel):\n",
    "    \"\"\"Call this tool to indicate that the research is complete.\"\"\"\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    summary: str\n",
    "    key_excerpts: str\n",
    "\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    need_clarification: bool = Field(\n",
    "        description = \"Whether the users need to be asked a clarifying question.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        desciption = \"A question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description = \"Verify message that we will start research after the user has provided the necessary information.\"\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    research_brief: str = Field(\n",
    "        description = \"A research question that will be used to guide the research.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "047e9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_reducer(current_value, new_value):\n",
    "    if isinstance(new_value, dict) and new_value.get(\"type\") == \"override\":\n",
    "        return new_value.get(\"value\", new_value)\n",
    "    else:\n",
    "        return operator.add(current_value, new_value) \n",
    "\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"InputState is only 'messages'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a6e3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(MessagesState):\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]\n",
    "    research_brief: Optional[str]\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []\n",
    "    notes: Annotated[list[str], override_reducer] = []\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorState(TypedDict):\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]\n",
    "    research_brief: str\n",
    "    notes: Annotated[list[str], override_reducer] = []\n",
    "    research_iterations: int = 0\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "    research`er_messages: Annotated[list[MessageLikeRepresentation], operator.add]\n",
    "    tool_call_iterations: int = 0\n",
    "    research_topic: str\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []\n",
    "\n",
    "class ResearcherOutputState(BaseModel):\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "282c2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Annotated, List, Literal, Dict, Optional, Any\n",
    "from langchain_core.tools import BaseTool, StructuredTool, tool, ToolException, InjectedToolArg\n",
    "from langchain_core.messages import HumanMessage, AIMessage, MessageLikeRepresentation, filter_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from tavily import AsyncTavilyClient\n",
    "from langgraph.config import get_store\n",
    "from mcp import McpError\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from src.open_deep_research.state import Summary, ResearchComplete\n",
    "from src.open_deep_research.configuration import SearchAPI, Configuration\n",
    "from src.open_deep_research.prompts import summarize_webpage_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9460990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily Search Tool Utils \n",
    "\n",
    "TAVILY_SEARCH_DESCRIPTION = (\n",
    "    \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n",
    "    \"Useful for when you need to answer questions about current events.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=TAVILY_SEARCH_DESCRIPTION)\n",
    "async def tavily_search(\n",
    "    queries: List[str],\n",
    "    max_results: Annotated[int, InjectedToolArg] = 5, \n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArgs] = \"general\",\n",
    "    config: RunnableConfig = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetches results from Tavily search API.\n",
    "\n",
    "    Args\n",
    "        queries (List[str]): List of search queries, you can pass in as many queries as you need.\n",
    "        max_results (int): Maximum number of results to return\n",
    "        topic (Literal['general', 'news', 'finance']): Topic to filter results by\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of search results\n",
    "    \"\"\"\n",
    "    search_results = await tavily_search_async(\n",
    "        queries,\n",
    "        max_results = max_results,\n",
    "        topic = topic,\n",
    "        include_raw_content = True,\n",
    "        config = config\n",
    "    )\n",
    "    formatted_output = f\"Search results: \\n\\n\"\n",
    "    unique_results = {}\n",
    "    for response in search_results:\n",
    "        for result in response['results']:\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = {**result, \"query\": response['query']}\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    max_char_to_include = 50000\n",
    "    model_api_key = get_api_key_for_model(configurable.summarization_model, config)\n",
    "    summarization_model = init_chat_model(\n",
    "        model=configurable.summarization_model,\n",
    "        max_tokens=configurable.summarization_model_max_tokens,\n",
    "        api_key=model_api_key,\n",
    "        tags=[\"langsmith:nostream\"]\n",
    "        ).with_structured_output(Summary).with_retry(stop_after_attempt=configurable.max_structured_output_retries)\n",
    "    async def noop():\n",
    "        return None\n",
    "\n",
    "    # * import * -> async / await으로 비동기적으로 summarization을 진행\n",
    "    summarization_tasks = [\n",
    "        noop() if not result.get(\"raw_content\") else summarize_webpage(summarization_model, \n",
    "        result['raw_content'][:max_char_to_include],\n",
    "        )\n",
    "        for result in unique_results.values()\n",
    "    ]\n",
    "    summaries = await asyncio.gather(*summarization_tasks)\n",
    "    summarized_results = {\n",
    "        url: {'title': result['title'], 'content': result['content'] if summary is none else summary}\n",
    "        for url, result, summary in zip(unique_results.keys(), unique_results.values(), summaries)\n",
    "    }\n",
    "    for i, (url, result) in enumerate(summarized_results.items()):\n",
    "        formatted_output +=f\"\\n\\n--- SOURCE {i+1}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        formatted_output += \"\\n\\n\" + \"-\" * 80 + \"\\n\"\n",
    "    if summarized_results:\n",
    "        return formatted_output\n",
    "    \n",
    "    else:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def tavily_search_async(search_queries, max_results: int = 5, topic: Literal['general','news','finance'] = 'general', include_raw_content: bool = True, config: RunnableConfig = None):\n",
    "    tavily_async_client = AsyncTavilyClient(api_key=get_tavily_api_key(config))\n",
    "    search_tasks = []\n",
    "    for query in search_queries:\n",
    "        tavily_async_client.search(\n",
    "            query,\n",
    "            max_results = max_results,\n",
    "            include_raw_content = include_raw_content,\n",
    "            topic = topic\n",
    "        )\n",
    "    search_docs = await asyncio.gather(*search_tasks)\n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac73528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def summarize_webpage(model: BaseChatModel, webpage_content: str) -> str:\n",
    "    try:\n",
    "        summary = await asyncio.wait_for(\n",
    "            model.ainvoke([HumanMessage(content=summarize_webpage_prompt.format(webpage_content=webpage_content, date=get_todat_str()))]),\n",
    "            timeout = 60\n",
    "        )\n",
    "        return f\"\"\"<summary>\\n{summary.summary}\\n</summary>\\n\\n<key_excerpts>\\n{summary.key_excerpts}\\n</key_excerpts>\"\"\"\n",
    "\n",
    "    except (asyncio.TimeoutError, Exception) as e:\n",
    "        print(f\"Failed to summarize webpage: {str(e)}\")\n",
    "        return webpage_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49cf1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.config import get_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_tokens(config: RunnableConfig):\n",
    "    store = get_store()\n",
    "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "311f8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": \n",
    "            {\"thread_id\": \"conversation-abc123\",\n",
    "            \"user_id\": \"user-456\",\n",
    "            \"session_id\": \"session-789\"}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a28b2714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conversation-abc123'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get(\"configurable\").get(\"thread_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_mcp_tools(\n",
    "    config: RunnableConfig,\n",
    "    existing_tool_names: set[str],\n",
    ") -> list[BaseTool]:\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06b5a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable = Configuration.from_runnable_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "047f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_value(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    elif isinstance(value, dict):\n",
    "        return value\n",
    "    else:\n",
    "        return value.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6867c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'a'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_config_value({\"test\":'a'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015be13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
